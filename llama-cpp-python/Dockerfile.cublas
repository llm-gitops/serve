#### Stage 0 ####

FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

RUN apt-get update && apt-get upgrade -y \
    && apt-get install -y git build-essential \
    python3 python3-pip gcc wget \
    ocl-icd-opencl-dev opencl-headers clinfo \
    libclblast-dev libopenblas-dev \
    && mkdir -p /etc/OpenCL/vendors && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd

# setting build related env vars
ENV CUDA_DOCKER_ARCH=all
ENV LLAMA_CUBLAS=1

# Install llama-cpp-python (build with cuda)
RUN CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python[server]

#### Stage 1 ####

FROM ubuntu:22.04

RUN mkdir -p /etc/OpenCL/vendors && echo "libnvidia-opencl.so.1" > /etc/OpenCL/vendors/nvidia.icd

ENV HOST 0.0.0.0
ENV PORT 8000

COPY --from=0 /lib/x86_64-linux-gnu                     /lib/x86_64-linux-gnu 
COPY --from=0 /usr/bin/python3.10                       /usr/bin/python3.10
COPY --from=0 /usr/bin/python3                          /usr/bin/python3
COPY --from=0 /usr/lib/python3.10                       /usr/lib/python3.10 
COPY --from=0 /usr/local/lib/python3.10                 /usr/local/lib/python3.10 
COPY --from=0 /etc/ld.so.conf.d/000_cuda.conf           /etc/ld.so.conf.d/000_cuda.conf
COPY --from=0 /etc/ld.so.conf.d/nvidia.conf             /etc/ld.so.conf.d/nvidia.conf
COPY --from=0 /etc/ld.so.conf                           /etc/ld.so.conf
COPY --from=0 /usr/local/cuda/targets/x86_64-linux/lib  /usr/local/cuda/targets/x86_64-linux/lib

RUN rm /etc/ld.so.cache && ldconfig

ENTRYPOINT ["python3", "-m", "llama_cpp.server"]

## Example
## docker run --privileged --net=host --gpus all -e PORT 80 -v /models/llama-2-7b-chat.Q5_K_M.gguf:/models/model.bin ghcr.io/llm-gitops/serve/llama-cpp-python:v1.0.0-gpu --model /models/model.bin --n_gpu_layers 35